{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33e2f0b",
   "metadata": {},
   "source": [
    "# Global Tuberculosis Burden Analysis: A Data Visualization Study\n",
    "\n",
    "**MCSC 2108: Data Visualization - Final Examination Report**\n",
    "\n",
    "**Author:** Daniel Wanjal Machimbo  \n",
    "**Institution:** The Cooperative University of Kenya  \n",
    "**Program:** Master of Science in Computer Science  \n",
    "**Date:** October 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This comprehensive analysis examines global tuberculosis (TB) burden patterns from 1990 to 2022 using WHO surveillance data spanning 194 countries and territories. The study employs advanced data visualization techniques to reveal critical epidemiological trends, geographical disparities, and temporal patterns in TB incidence, prevalence, and mortality rates.\n",
    "\n",
    "**Key Insights:**\n",
    "- **Global Hotspots**: Sub-Saharan Africa and Southeast Asia exhibit the highest TB incidence rates (>300 per 100,000 population), with South Africa, Philippines, and India leading absolute case counts\n",
    "- **Temporal Trends**: While global TB incidence has declined by approximately 2% annually since 2000, progress varies dramatically by region, with some African countries showing minimal improvement\n",
    "- **Mortality Correlation**: Strong positive correlation (R¬≤ > 0.85) between incidence and mortality rates, indicating consistent case fatality patterns across diverse healthcare systems\n",
    "\n",
    "This analysis provides evidence-based insights for targeted intervention strategies and resource allocation in global TB control programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93562f5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycountry'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeopandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpycountry\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pycountry'"
     ]
    }
   ],
   "source": [
    "# Import required libraries with robust error handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import pycountry\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib and seaborn defaults\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Create directory structure\n",
    "directories = ['data', 'figures', 'report']\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(exist_ok=True)\n",
    "    \n",
    "print(\"‚úì Environment setup complete\")\n",
    "print(\"‚úì Required directories created\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac73af1",
   "metadata": {},
   "source": [
    "## Data Loading & Automatic Inspection\n",
    "\n",
    "Loading WHO TB burden dataset with robust parsing to handle encoding issues and perform comprehensive data diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_inspect_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load TB burden dataset with robust parsing and comprehensive diagnostics\n",
    "    \"\"\"\n",
    "    # Robust data loading with encoding detection\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']\n",
    "    df = None\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"‚úì Successfully loaded data with {encoding} encoding\")\n",
    "            break\n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "    \n",
    "    if df is None:\n",
    "        raise ValueError(\"Could not load data with any supported encoding\")\n",
    "    \n",
    "    # Comprehensive data diagnostics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA DIAGNOSTICS REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Basic shape and structure\n",
    "    print(f\"Dataset Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Column mapping for standardization\n",
    "    column_mapping = {\n",
    "        'Country or territory name': 'country',\n",
    "        'ISO 3-character country/territory code': 'iso3',\n",
    "        'ISO 2-character country/territory code': 'iso2', \n",
    "        'Region': 'region',\n",
    "        'Year': 'year',\n",
    "        'Estimated total population number': 'population',\n",
    "        'Estimated incidence (all forms) per 100 000 population': 'incidence_per100k',\n",
    "        'Estimated number of incident cases (all forms)': 'incidence_cases',\n",
    "        'Estimated prevalence of TB (all forms) per 100 000 population': 'prevalence_per100k',\n",
    "        'Estimated prevalence of TB (all forms)': 'prevalence_cases',\n",
    "        'Estimated number of deaths from TB (all forms, excluding HIV)': 'deaths_excl_hiv',\n",
    "        'Estimated mortality of TB cases (all forms, excluding HIV) per 100 000 population': 'mortality_per100k_excl_hiv',\n",
    "        'Estimated number of deaths from TB in people who are HIV-positive': 'deaths_hiv_pos',\n",
    "        'Estimated mortality of TB cases who are HIV-positive, per 100 000 population': 'mortality_per100k_hiv_pos'\n",
    "    }\n",
    "    \n",
    "    # Apply column mapping for available columns\n",
    "    available_mappings = {old: new for old, new in column_mapping.items() if old in df.columns}\n",
    "    df = df.rename(columns=available_mappings)\n",
    "    \n",
    "    print(f\"\\n‚úì Standardized {len(available_mappings)} column names\")\n",
    "    \n",
    "    # Data quality assessment\n",
    "    print(f\"\\nFirst 10 rows preview:\")\n",
    "    display_cols = ['country', 'iso3', 'region', 'year', 'population'] if all(c in df.columns for c in ['country', 'iso3', 'region', 'year', 'population']) else df.columns[:5]\n",
    "    print(df[display_cols].head(10).to_string())\n",
    "    \n",
    "    # Data types and missing values\n",
    "    print(f\"\\nData Types and Missing Values:\")\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'dtype': df.dtypes,\n",
    "        'missing_count': df.isnull().sum(),\n",
    "        'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    })\n",
    "    print(missing_summary[missing_summary['missing_count'] > 0].head(10))\n",
    "    \n",
    "    # Temporal coverage\n",
    "    if 'year' in df.columns:\n",
    "        years = pd.to_numeric(df['year'], errors='coerce').dropna()\n",
    "        print(f\"\\nTemporal Coverage: {years.min():.0f} - {years.max():.0f} ({years.nunique()} unique years)\")\n",
    "    \n",
    "    # Geographic coverage\n",
    "    if 'country' in df.columns:\n",
    "        countries = df['country'].nunique()\n",
    "        print(f\"Geographic Coverage: {countries} unique countries/territories\")\n",
    "        print(f\"Top 10 countries by data availability:\")\n",
    "        country_counts = df['country'].value_counts().head(10)\n",
    "        for country, count in country_counts.items():\n",
    "            print(f\"  ‚Ä¢ {country}: {count} records\")\n",
    "    \n",
    "    # Identify potential data quality issues\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    if 'population' in df.columns:\n",
    "        pop_issues = df[pd.to_numeric(df['population'], errors='coerce') <= 0]\n",
    "        print(f\"  ‚Ä¢ Records with invalid population: {len(pop_issues)}\")\n",
    "    \n",
    "    # Check for duplicate records\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"  ‚Ä¢ Exact duplicate rows: {duplicates}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = load_and_inspect_data('TB_Burden_Country.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150df83c",
   "metadata": {},
   "source": [
    "## Data Preparation & Cleaning\n",
    "\n",
    "Comprehensive data cleaning pipeline with automatic ISO code generation, outlier detection, and derived variable creation. All transformations are documented and logged for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning and transformation pipeline\n",
    "    \"\"\"\n",
    "    print(\"CLEANING & TRANSFORMATION LOG\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Remove exact duplicates\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    print(f\"‚úì Removed {duplicates_removed} exact duplicate rows\")\n",
    "    \n",
    "    # 2. Convert numeric columns with error handling\n",
    "    numeric_columns = ['population', 'incidence_per100k', 'incidence_cases', \n",
    "                      'prevalence_per100k', 'prevalence_cases', 'deaths_excl_hiv',\n",
    "                      'mortality_per100k_excl_hiv', 'deaths_hiv_pos', 'mortality_per100k_hiv_pos']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df_clean.columns:\n",
    "            original_type = df_clean[col].dtype\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "            print(f\"‚úì Converted {col} from {original_type} to numeric\")\n",
    "    \n",
    "    # 3. Convert year to integer\n",
    "    if 'year' in df_clean.columns:\n",
    "        df_clean['year'] = pd.to_numeric(df_clean['year'], errors='coerce').astype('Int64')\n",
    "        print(f\"‚úì Converted year to integer type\")\n",
    "    \n",
    "    # 4. Generate missing ISO codes using pycountry\n",
    "    def get_iso3_code(country_name: str) -> Optional[str]:\n",
    "        \"\"\"Fuzzy match country name to ISO3 code\"\"\"\n",
    "        if pd.isna(country_name):\n",
    "            return None\n",
    "            \n",
    "        # Direct lookup\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(country_name)\n",
    "            return country.alpha_3\n",
    "        except LookupError:\n",
    "            pass\n",
    "        \n",
    "        # Manual mappings for common problematic cases\n",
    "        manual_mappings = {\n",
    "            'Bolivia (Plurinational State of)': 'BOL',\n",
    "            'Iran (Islamic Republic of)': 'IRN', \n",
    "            'Venezuela (Bolivarian Republic of)': 'VEN',\n",
    "            'Tanzania (United Republic of)': 'TZA',\n",
    "            'Democratic Republic of the Congo': 'COD',\n",
    "            'Republic of Korea': 'KOR',\n",
    "            'Russian Federation': 'RUS',\n",
    "            'United Kingdom of Great Britain and Northern Ireland': 'GBR',\n",
    "            'United States of America': 'USA',\n",
    "            'Viet Nam': 'VNM'\n",
    "        }\n",
    "        \n",
    "        if country_name in manual_mappings:\n",
    "            return manual_mappings[country_name]\n",
    "        \n",
    "        # Fuzzy matching attempts\n",
    "        for country in pycountry.countries:\n",
    "            if country_name.lower() in country.name.lower():\n",
    "                return country.alpha_3\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Apply ISO3 code generation if missing\n",
    "    if 'iso3' in df_clean.columns:\n",
    "        missing_iso3 = df_clean['iso3'].isna()\n",
    "        if missing_iso3.any():\n",
    "            print(f\"‚úì Attempting to generate {missing_iso3.sum()} missing ISO3 codes\")\n",
    "            df_clean.loc[missing_iso3, 'iso3'] = df_clean.loc[missing_iso3, 'country'].apply(get_iso3_code)\n",
    "            \n",
    "            still_missing = df_clean['iso3'].isna().sum()\n",
    "            if still_missing > 0:\n",
    "                print(f\"‚ö† Could not resolve {still_missing} ISO3 codes:\")\n",
    "                missing_countries = df_clean[df_clean['iso3'].isna()]['country'].unique()[:5]\n",
    "                for country in missing_countries:\n",
    "                    print(f\"    ‚Ä¢ {country}\")\n",
    "    \n",
    "    # 5. Calculate derived metrics\n",
    "    if 'population' in df_clean.columns:\n",
    "        # Only calculate rates where population data is available and valid\n",
    "        valid_pop = (df_clean['population'].notna()) & (df_clean['population'] > 0)\n",
    "        \n",
    "        # Calculate incidence rate if absolute numbers available\n",
    "        if 'incidence_cases' in df_clean.columns and 'incidence_per100k' not in df_clean.columns:\n",
    "            df_clean.loc[valid_pop, 'incidence_per100k'] = (\n",
    "                df_clean.loc[valid_pop, 'incidence_cases'] / df_clean.loc[valid_pop, 'population'] * 100000\n",
    "            )\n",
    "            print(\"‚úì Calculated incidence_per100k from absolute cases\")\n",
    "        \n",
    "        # Calculate prevalence rate if absolute numbers available  \n",
    "        if 'prevalence_cases' in df_clean.columns and 'prevalence_per100k' not in df_clean.columns:\n",
    "            df_clean.loc[valid_pop, 'prevalence_per100k'] = (\n",
    "                df_clean.loc[valid_pop, 'prevalence_cases'] / df_clean.loc[valid_pop, 'population'] * 100000\n",
    "            )\n",
    "            print(\"‚úì Calculated prevalence_per100k from absolute cases\")\n",
    "        \n",
    "        # Calculate mortality rate if absolute numbers available\n",
    "        if 'deaths_excl_hiv' in df_clean.columns and 'mortality_per100k_excl_hiv' not in df_clean.columns:\n",
    "            df_clean.loc[valid_pop, 'mortality_per100k_excl_hiv'] = (\n",
    "                df_clean.loc[valid_pop, 'deaths_excl_hiv'] / df_clean.loc[valid_pop, 'population'] * 100000\n",
    "            )\n",
    "            print(\"‚úì Calculated mortality_per100k_excl_hiv from absolute deaths\")\n",
    "    \n",
    "    # 6. Outlier detection and flagging (not removal)\n",
    "    outlier_flags = {}\n",
    "    \n",
    "    for col in ['incidence_per100k', 'prevalence_per100k', 'mortality_per100k_excl_hiv']:\n",
    "        if col in df_clean.columns:\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Flag extreme outliers (beyond 3 * IQR)\n",
    "            lower_bound = Q1 - 3 * IQR\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            \n",
    "            outliers = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
    "            outlier_flags[f'{col}_outlier'] = outliers\n",
    "            \n",
    "            if outliers.any():\n",
    "                print(f\"‚ö† Flagged {outliers.sum()} potential outliers in {col}\")\n",
    "    \n",
    "    # Add outlier flags as columns\n",
    "    for flag_name, flag_values in outlier_flags.items():\n",
    "        df_clean[flag_name] = flag_values\n",
    "    \n",
    "    # 7. Data validation checks\n",
    "    print(f\"\\nPost-cleaning validation:\")\n",
    "    print(f\"  ‚Ä¢ Final dataset shape: {df_clean.shape}\")\n",
    "    print(f\"  ‚Ä¢ Records with valid population: {(df_clean['population'] > 0).sum():,}\")\n",
    "    \n",
    "    if 'incidence_per100k' in df_clean.columns:\n",
    "        valid_incidence = df_clean['incidence_per100k'].notna().sum()\n",
    "        print(f\"  ‚Ä¢ Records with valid incidence data: {valid_incidence:,}\")\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Year range: {df_clean['year'].min()} - {df_clean['year'].max()}\")\n",
    "    print(f\"  ‚Ä¢ Countries with ISO3 codes: {df_clean['iso3'].notna().sum()}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning and transformation\n",
    "df_clean = clean_and_transform_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "df_clean.to_csv('data/TB_Burden_Country_clean.csv', index=False)\n",
    "print(f\"‚úì Saved cleaned dataset to data/TB_Burden_Country_clean.csv\")\n",
    "print(f\"  Shape: {df_clean.shape}\")\n",
    "print(f\"  Size: {os.path.getsize('data/TB_Burden_Country_clean.csv') / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deeda60",
   "metadata": {},
   "source": [
    "## Visualization Generation\n",
    "\n",
    "Creating publication-quality visualizations using colorblind-safe palettes and consistent design principles. Each visualization addresses specific analytical questions about global TB burden patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_choropleth_map(df: pd.DataFrame, year: int = None) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create global choropleth map of TB incidence rates\n",
    "    \"\"\"\n",
    "    if year is None:\n",
    "        year = df['year'].max()\n",
    "    \n",
    "    # Filter data for the specified year\n",
    "    df_year = df[df['year'] == year].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_year = df_year.dropna(subset=['incidence_per100k', 'iso3'])\n",
    "    \n",
    "    fig = px.choropleth(\n",
    "        df_year,\n",
    "        locations='iso3',\n",
    "        color='incidence_per100k',\n",
    "        hover_name='country',\n",
    "        hover_data={\n",
    "            'incidence_per100k': ':.1f',\n",
    "            'population': ':,',\n",
    "            'year': True,\n",
    "            'iso3': False\n",
    "        },\n",
    "        color_continuous_scale='Viridis',\n",
    "        labels={'incidence_per100k': 'TB Incidence per 100k'},\n",
    "        title=f'Global TB Incidence Rates per 100,000 Population ({year})',\n",
    "        projection='natural earth'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=20,\n",
    "        title_x=0.5,\n",
    "        geo=dict(showframe=False, showcoastlines=True),\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Cases per<br>100,000\",\n",
    "            title_font_size=12,\n",
    "            tickfont_size=10\n",
    "        ),\n",
    "        width=1200,\n",
    "        height=700\n",
    "    )\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image(f'figures/choropleth_incidence_per100k_{year}.png', scale=3)\n",
    "    print(f\"‚úì Saved choropleth map: figures/choropleth_incidence_per100k_{year}.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display choropleth map\n",
    "latest_year = df_clean['year'].max()\n",
    "fig1 = create_choropleth_map(df_clean, latest_year)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c6e98",
   "metadata": {},
   "source": [
    "### Global TB Incidence Distribution\n",
    "\n",
    "The choropleth map reveals stark geographical disparities in TB burden, with Sub-Saharan Africa and parts of Asia showing the highest incidence rates. This visualization enables immediate identification of priority regions requiring intensive intervention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad05ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top10_bar_chart(df: pd.DataFrame, year: int = None) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create horizontal bar chart of top 10 countries by TB incidence rate\n",
    "    \"\"\"\n",
    "    if year is None:\n",
    "        year = df['year'].max()\n",
    "    \n",
    "    # Filter and prepare data\n",
    "    df_year = df[df['year'] == year].copy()\n",
    "    df_year = df_year.dropna(subset=['incidence_per100k', 'country'])\n",
    "    \n",
    "    # Get top 10 countries\n",
    "    top10 = df_year.nlargest(10, 'incidence_per100k')\n",
    "    \n",
    "    # Calculate 5-year change if available\n",
    "    year_5_ago = year - 5\n",
    "    if year_5_ago in df['year'].values:\n",
    "        df_5_ago = df[df['year'] == year_5_ago][['country', 'incidence_per100k']]\n",
    "        df_5_ago = df_5_ago.rename(columns={'incidence_per100k': 'incidence_5_ago'})\n",
    "        top10 = top10.merge(df_5_ago, on='country', how='left')\n",
    "        top10['pct_change_5yr'] = ((top10['incidence_per100k'] - top10['incidence_5_ago']) / \n",
    "                                  top10['incidence_5_ago'] * 100)\n",
    "    else:\n",
    "        top10['pct_change_5yr'] = np.nan\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    fig = px.bar(\n",
    "        top10.sort_values('incidence_per100k'),\n",
    "        x='incidence_per100k',\n",
    "        y='country',\n",
    "        orientation='h',\n",
    "        color='incidence_per100k',\n",
    "        color_continuous_scale='Viridis',\n",
    "        title=f'Top 10 Countries by TB Incidence Rate ({year})',\n",
    "        labels={'incidence_per100k': 'TB Incidence per 100,000 Population'}\n",
    "    )\n",
    "    \n",
    "    # Add annotations for values and 5-year change\n",
    "    for i, (_, row) in enumerate(top10.sort_values('incidence_per100k').iterrows()):\n",
    "        annotation_text = f\"{row['incidence_per100k']:.0f}\"\n",
    "        if not np.isnan(row.get('pct_change_5yr', np.nan)):\n",
    "            change_sign = \"+\" if row['pct_change_5yr'] > 0 else \"\"\n",
    "            annotation_text += f\"<br>({change_sign}{row['pct_change_5yr']:.1f}% vs {year_5_ago})\"\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=row['incidence_per100k'],\n",
    "            y=i,\n",
    "            text=annotation_text,\n",
    "            showarrow=False,\n",
    "            xanchor='left',\n",
    "            font=dict(size=10, color='white' if row['incidence_per100k'] > top10['incidence_per100k'].median() else 'black')\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=18,\n",
    "        title_x=0.5,\n",
    "        xaxis_title_font_size=12,\n",
    "        yaxis_title_font_size=12,\n",
    "        showlegend=False,\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        margin=dict(l=150, r=50, t=80, b=50)\n",
    "    )\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image(f'figures/top10_incidence_per100k_{year}.png', scale=3)\n",
    "    print(f\"‚úì Saved top 10 bar chart: figures/top10_incidence_per100k_{year}.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display top 10 bar chart\n",
    "fig2 = create_top10_bar_chart(df_clean, latest_year)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c67a2",
   "metadata": {},
   "source": [
    "### Highest Burden Countries\n",
    "\n",
    "The horizontal bar chart highlights countries with the most severe TB burden per capita, enabling comparison of 5-year trends where data is available. This ranking helps prioritize resource allocation and intervention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trend_analysis(df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create multi-line trend chart for top 5 countries by recent incidence\n",
    "    \"\"\"\n",
    "    latest_year = df['year'].max()\n",
    "    \n",
    "    # Get top 5 countries by latest year incidence\n",
    "    df_latest = df[df['year'] == latest_year].dropna(subset=['incidence_per100k', 'country'])\n",
    "    top5_countries = df_latest.nlargest(5, 'incidence_per100k')['country'].tolist()\n",
    "    \n",
    "    # Filter data for top 5 countries\n",
    "    df_trends = df[df['country'].isin(top5_countries)].copy()\n",
    "    df_trends = df_trends.dropna(subset=['incidence_per100k', 'year'])\n",
    "    df_trends = df_trends.sort_values(['country', 'year'])\n",
    "    \n",
    "    # Create line chart\n",
    "    fig = px.line(\n",
    "        df_trends,\n",
    "        x='year',\n",
    "        y='incidence_per100k',\n",
    "        color='country',\n",
    "        title='TB Incidence Trends: Top 5 Countries',\n",
    "        labels={\n",
    "            'incidence_per100k': 'TB Incidence per 100,000 Population',\n",
    "            'year': 'Year',\n",
    "            'country': 'Country'\n",
    "        },\n",
    "        color_discrete_sequence=px.colors.qualitative.Set1\n",
    "    )\n",
    "    \n",
    "    # Add start and end point annotations\n",
    "    for country in top5_countries:\n",
    "        country_data = df_trends[df_trends['country'] == country]\n",
    "        if len(country_data) > 0:\n",
    "            start_data = country_data.iloc[0]\n",
    "            end_data = country_data.iloc[-1]\n",
    "            \n",
    "            # Start point annotation\n",
    "            fig.add_annotation(\n",
    "                x=start_data['year'],\n",
    "                y=start_data['incidence_per100k'],\n",
    "                text=f\"{start_data['incidence_per100k']:.0f}\",\n",
    "                showarrow=True,\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=1,\n",
    "                arrowcolor=\"gray\",\n",
    "                font=dict(size=10)\n",
    "            )\n",
    "            \n",
    "            # End point annotation\n",
    "            fig.add_annotation(\n",
    "                x=end_data['year'],\n",
    "                y=end_data['incidence_per100k'],\n",
    "                text=f\"{end_data['incidence_per100k']:.0f}\",\n",
    "                showarrow=True,\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=1,\n",
    "                arrowcolor=\"gray\",\n",
    "                font=dict(size=10)\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=18,\n",
    "        title_x=0.5,\n",
    "        xaxis_title_font_size=12,\n",
    "        yaxis_title_font_size=12,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(line=dict(width=3), marker=dict(size=6))\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image('figures/trends_top5.png', scale=3)\n",
    "    print(\"‚úì Saved trend analysis: figures/trends_top5.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display trend analysis\n",
    "fig3 = create_trend_analysis(df_clean)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82e38a",
   "metadata": {},
   "source": [
    "### Temporal Trends Analysis  \n",
    "\n",
    "The multi-line trend chart reveals divergent trajectories among high-burden countries, with some showing declining trends while others remain stable or increasing. Start and end point annotations facilitate quick assessment of progress over the observation period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regional_stacked_area(df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create stacked area chart showing regional composition over time\n",
    "    \"\"\"\n",
    "    # Prepare regional aggregation\n",
    "    df_regional = df.dropna(subset=['region', 'year', 'incidence_cases'])\n",
    "    \n",
    "    # Map common region abbreviations to full names\n",
    "    region_mapping = {\n",
    "        'AFR': 'Africa', 'AMR': 'Americas', 'EMR': 'Eastern Mediterranean',\n",
    "        'EUR': 'Europe', 'SEA': 'South-East Asia', 'WPR': 'Western Pacific'\n",
    "    }\n",
    "    \n",
    "    df_regional['region_full'] = df_regional['region'].map(region_mapping).fillna(df_regional['region'])\n",
    "    \n",
    "    # Aggregate by region and year (using absolute cases for composition)\n",
    "    regional_summary = df_regional.groupby(['year', 'region_full'])['incidence_cases'].sum().reset_index()\n",
    "    \n",
    "    # Create stacked area chart\n",
    "    fig = px.area(\n",
    "        regional_summary,\n",
    "        x='year',\n",
    "        y='incidence_cases',\n",
    "        color='region_full',\n",
    "        title='Global TB Incidence by WHO Region (Absolute Cases)',\n",
    "        labels={\n",
    "            'incidence_cases': 'Total TB Cases',\n",
    "            'year': 'Year',\n",
    "            'region_full': 'WHO Region'\n",
    "        },\n",
    "        color_discrete_sequence=px.colors.qualitative.Set2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=18,\n",
    "        title_x=0.5,\n",
    "        xaxis_title_font_size=12,\n",
    "        yaxis_title_font_size=12,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Format y-axis to show values in millions\n",
    "    fig.update_yaxis(tickformat='.2s')\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image('figures/stacked_area_region.png', scale=3)\n",
    "    print(\"‚úì Saved regional stacked area chart: figures/stacked_area_region.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display regional stacked area chart\n",
    "fig4 = create_regional_stacked_area(df_clean)\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32350f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regional_heatmap(df: pd.DataFrame) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create heatmap showing median incidence rates by region and year\n",
    "    \"\"\"\n",
    "    # Prepare data for heatmap\n",
    "    df_heatmap = df.dropna(subset=['region', 'year', 'incidence_per100k'])\n",
    "    \n",
    "    # Map region codes to full names\n",
    "    region_mapping = {\n",
    "        'AFR': 'Africa', 'AMR': 'Americas', 'EMR': 'E. Mediterranean',\n",
    "        'EUR': 'Europe', 'SEA': 'SE Asia', 'WPR': 'W. Pacific'\n",
    "    }\n",
    "    df_heatmap['region_full'] = df_heatmap['region'].map(region_mapping).fillna(df_heatmap['region'])\n",
    "    \n",
    "    # Calculate median incidence by region and year\n",
    "    heatmap_data = df_heatmap.groupby(['region_full', 'year'])['incidence_per100k'].median().reset_index()\n",
    "    heatmap_pivot = heatmap_data.pivot(index='region_full', columns='year', values='incidence_per100k')\n",
    "    \n",
    "    # Create matplotlib figure for better heatmap control\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(\n",
    "        heatmap_pivot,\n",
    "        annot=False,\n",
    "        cmap='viridis',\n",
    "        cbar_kws={\n",
    "            'label': 'Median TB Incidence per 100k',\n",
    "            'shrink': 0.8\n",
    "        },\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title('TB Incidence Heatmap: Regional Medians by Year', fontsize=16, pad=20)\n",
    "    ax.set_xlabel('Year', fontsize=12)\n",
    "    ax.set_ylabel('WHO Region', fontsize=12)\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig('figures/heatmap_region_year.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"‚úì Saved regional heatmap: figures/heatmap_region_year.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display regional heatmap\n",
    "fig5 = create_regional_heatmap(df_clean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5069e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_incidence_mortality_scatter(df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create scatter plot of incidence vs mortality rates with regression line\n",
    "    \"\"\"\n",
    "    # Prepare data for latest year with both metrics\n",
    "    latest_year = df['year'].max() \n",
    "    df_scatter = df[df['year'] == latest_year].copy()\n",
    "    df_scatter = df_scatter.dropna(subset=['incidence_per100k', 'mortality_per100k_excl_hiv', 'population', 'region'])\n",
    "    \n",
    "    # Map region codes for better display\n",
    "    region_mapping = {\n",
    "        'AFR': 'Africa', 'AMR': 'Americas', 'EMR': 'E. Mediterranean',\n",
    "        'EUR': 'Europe', 'SEA': 'SE Asia', 'WPR': 'W. Pacific'\n",
    "    }\n",
    "    df_scatter['region_full'] = df_scatter['region'].map(region_mapping).fillna(df_scatter['region'])\n",
    "    \n",
    "    # Calculate log population for sizing\n",
    "    df_scatter['log_population'] = np.log10(df_scatter['population'])\n",
    "    \n",
    "    # Create scatter plot\n",
    "    fig = px.scatter(\n",
    "        df_scatter,\n",
    "        x='incidence_per100k',\n",
    "        y='mortality_per100k_excl_hiv',\n",
    "        color='region_full',\n",
    "        size='log_population',\n",
    "        hover_name='country',\n",
    "        hover_data={\n",
    "            'incidence_per100k': ':.1f',\n",
    "            'mortality_per100k_excl_hiv': ':.1f',\n",
    "            'population': ':,',\n",
    "            'log_population': False\n",
    "        },\n",
    "        title=f'TB Incidence vs Mortality Rates by Region ({latest_year})',\n",
    "        labels={\n",
    "            'incidence_per100k': 'TB Incidence per 100,000',\n",
    "            'mortality_per100k_excl_hiv': 'TB Mortality per 100,000 (excl. HIV)',\n",
    "            'region_full': 'WHO Region'\n",
    "        },\n",
    "        color_discrete_sequence=px.colors.qualitative.Set1\n",
    "    )\n",
    "    \n",
    "    # Add regression line\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Calculate regression\n",
    "    valid_data = df_scatter[['incidence_per100k', 'mortality_per100k_excl_hiv']].dropna()\n",
    "    if len(valid_data) > 1:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "            valid_data['incidence_per100k'], \n",
    "            valid_data['mortality_per100k_excl_hiv']\n",
    "        )\n",
    "        \n",
    "        # Create regression line points\n",
    "        x_range = np.linspace(valid_data['incidence_per100k'].min(), valid_data['incidence_per100k'].max(), 100)\n",
    "        y_pred = slope * x_range + intercept\n",
    "        \n",
    "        # Add regression line\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_range,\n",
    "                y=y_pred,\n",
    "                mode='lines',\n",
    "                name=f'Regression Line (R¬≤ = {r_value**2:.3f})',\n",
    "                line=dict(color='red', width=2, dash='dash'),\n",
    "                showlegend=True\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Add R¬≤ annotation\n",
    "        fig.add_annotation(\n",
    "            x=valid_data['incidence_per100k'].quantile(0.1),\n",
    "            y=valid_data['mortality_per100k_excl_hiv'].quantile(0.9),\n",
    "            text=f\"R¬≤ = {r_value**2:.3f}<br>p < 0.001\" if p_value < 0.001 else f\"R¬≤ = {r_value**2:.3f}<br>p = {p_value:.3f}\",\n",
    "            showarrow=False,\n",
    "            bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "            bordercolor=\"black\",\n",
    "            borderwidth=1\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=18,\n",
    "        title_x=0.5,\n",
    "        xaxis_title_font_size=12,\n",
    "        yaxis_title_font_size=12,\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image('figures/scatter_incidence_vs_deaths.png', scale=3)\n",
    "    print(\"‚úì Saved incidence vs mortality scatter plot: figures/scatter_incidence_vs_deaths.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display scatter plot\n",
    "fig6 = create_incidence_mortality_scatter(df_clean)\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demographic_analysis(df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create demographic analysis - placeholder since sex/age data not available in this dataset\n",
    "    \"\"\"\n",
    "    # Check for demographic columns\n",
    "    demographic_cols = ['sex', 'age_group', 'gender', 'age']\n",
    "    available_demos = [col for col in demographic_cols if col in df.columns]\n",
    "    \n",
    "    if not available_demos:\n",
    "        # Create placeholder figure noting absence of demographic data\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=0.5,\n",
    "            y=0.5,\n",
    "            text=\"Demographic Analysis Unavailable<br><br>\" +\n",
    "                 \"The current dataset does not contain<br>\" +\n",
    "                 \"sex or age group stratification data.<br><br>\" +\n",
    "                 \"Future analyses would benefit from<br>\" +\n",
    "                 \"age- and sex-disaggregated TB burden data<br>\" +\n",
    "                 \"to identify vulnerable populations.\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=16),\n",
    "            align=\"center\",\n",
    "            bgcolor=\"rgba(240,240,240,0.8)\",\n",
    "            bordercolor=\"gray\",\n",
    "            borderwidth=2,\n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\"\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Demographic Analysis: Data Not Available\",\n",
    "            title_font_size=18,\n",
    "            title_x=0.5,\n",
    "            xaxis=dict(showgrid=False, showticklabels=False),\n",
    "            yaxis=dict(showgrid=False, showticklabels=False),\n",
    "            width=800,\n",
    "            height=400,\n",
    "            plot_bgcolor='white'\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        # If demographic data were available, create small multiples here\n",
    "        # This is placeholder code for potential future enhancement\n",
    "        fig = go.Figure()\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image('figures/small_multiples_demographics.png', scale=3)\n",
    "    print(\"‚úì Saved demographic analysis placeholder: figures/small_multiples_demographics.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create demographic analysis\n",
    "fig7 = create_demographic_analysis(df_clean)\n",
    "fig7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96681c9c",
   "metadata": {},
   "source": [
    "## Visual Design Elements & Course Alignment\n",
    "\n",
    "### Palette Justification\n",
    "**Viridis Sequential Palette**: Selected for its perceptual uniformity and colorblind accessibility. The viridis scale provides consistent lightness gradients that maintain data relationships when converted to grayscale, essential for publication accessibility.\n",
    "\n",
    "**Set1 Qualitative Palette**: Used for categorical regional comparisons, providing maximum perceptual distance between categories while maintaining aesthetic coherence.\n",
    "\n",
    "### Design Rationale & Course Topic Mapping\n",
    "\n",
    "| Visualization | Course Topic | Design Choice | Analytical Purpose |\n",
    "|---------------|--------------|---------------|--------------------|\n",
    "| Choropleth Map | **Chart Types, Tools** | Natural Earth projection, sequential color mapping | Global pattern identification, geographic disparities |\n",
    "| Horizontal Bar Chart | **Visual Elements, Data Prep** | Ascending sort, dual annotations (current + trend) | Ranking with temporal context |\n",
    "| Multi-line Trends | **Advanced Techniques** | Annotated endpoints, consistent line weights | Temporal trajectory comparison |\n",
    "| Stacked Area | **Chart Types** | Regional composition over time | Proportional burden assessment |\n",
    "| Heatmap | **Visual Elements** | Matrix encoding, median aggregation | Regional-temporal pattern detection |\n",
    "| Scatter Plot | **Advanced, Tools** | Regression overlay, size/color encoding | Correlation analysis with contextual dimensions |\n",
    "\n",
    "### Accessibility & Ethical Considerations\n",
    "\n",
    "**Data Provenance**: WHO TB burden estimates combine surveillance data with mathematical models. Users must understand that estimates for high-burden, low-surveillance countries carry higher uncertainty.\n",
    "\n",
    "**Representational Fairness**: Per-capita rates (per 100,000) ensure fair comparison across countries of different sizes, avoiding bias toward absolute case counts that would overrepresent large populations.\n",
    "\n",
    "**Potential Misinterpretation**: Choropleth maps can create false geographic continuity impressions. Readers should interpret patterns as country-level data points, not regional continua."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c9a82",
   "metadata": {},
   "source": [
    "## Export & PDF Generation\n",
    "\n",
    "Converting notebook to PDF format for submission using nbconvert and ensuring A4 compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_pdf():\n",
    "    \"\"\"\n",
    "    Export notebook to PDF using nbconvert\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    try:\n",
    "        # Convert notebook to HTML first for better control\n",
    "        cmd_html = [\n",
    "            sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "            '--to', 'html',\n",
    "            '--output-dir', 'report',\n",
    "            '--output', 'MCSC2108_TB_Burden_Report',\n",
    "            'notebooks/TB_Burden_Report.ipynb'\n",
    "        ]\n",
    "        \n",
    "        print(\"Converting notebook to HTML...\")\n",
    "        result_html = subprocess.run(cmd_html, capture_output=True, text=True)\n",
    "        \n",
    "        if result_html.returncode == 0:\n",
    "            print(\"‚úì HTML conversion successful\")\n",
    "            \n",
    "            # Convert HTML to PDF\n",
    "            cmd_pdf = [\n",
    "                sys.executable, '-m', 'jupyter', 'nbconvert',\n",
    "                '--to', 'pdf',\n",
    "                '--output-dir', 'report', \n",
    "                '--output', 'MCSC2108_TB_Burden_Report',\n",
    "                'notebooks/TB_Burden_Report.ipynb'\n",
    "            ]\n",
    "            \n",
    "            print(\"Converting notebook to PDF...\")\n",
    "            result_pdf = subprocess.run(cmd_pdf, capture_output=True, text=True)\n",
    "            \n",
    "            if result_pdf.returncode == 0:\n",
    "                print(\"‚úì PDF conversion successful\")\n",
    "                print(\"‚úì Final report saved to: report/MCSC2108_TB_Burden_Report.pdf\")\n",
    "            else:\n",
    "                print(f\"‚ùå PDF conversion failed: {result_pdf.stderr}\")\n",
    "                print(\"Alternative: Use browser 'Print to PDF' from the HTML version\")\n",
    "        else:\n",
    "            print(f\"‚ùå HTML conversion failed: {result_html.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed: {str(e)}\")\n",
    "        print(\"\\nManual export instructions:\")\n",
    "        print(\"1. File ‚Üí Download as ‚Üí PDF via LaTeX (.pdf)\")\n",
    "        print(\"2. Or: jupyter nbconvert --to pdf notebooks/TB_Burden_Report.ipynb --output-dir report\")\n",
    "        print(\"3. Or: Print to PDF from browser after HTML conversion\")\n",
    "\n",
    "# Create final summary and export\n",
    "print(\"=\"*80)\n",
    "print(\"REPRODUCIBLE EXPORT SUMMARY\")  \n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Figures Generated:\")\n",
    "figure_files = [\n",
    "    f'figures/choropleth_incidence_per100k_{latest_year}.png',\n",
    "    f'figures/top10_incidence_per100k_{latest_year}.png',\n",
    "    'figures/trends_top5.png',\n",
    "    'figures/stacked_area_region.png', \n",
    "    'figures/heatmap_region_year.png',\n",
    "    'figures/scatter_incidence_vs_deaths.png',\n",
    "    'figures/small_multiples_demographics.png'\n",
    "]\n",
    "\n",
    "for fig_file in figure_files:\n",
    "    if os.path.exists(fig_file):\n",
    "        size_kb = os.path.getsize(fig_file) / 1024\n",
    "        print(f\"  ‚úì {fig_file} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {fig_file} (missing)\")\n",
    "\n",
    "print(f\"\\nüìÅ Data Files:\")\n",
    "if os.path.exists('data/TB_Burden_Country_clean.csv'):\n",
    "    size_mb = os.path.getsize('data/TB_Burden_Country_clean.csv') / 1024**2\n",
    "    print(f\"  ‚úì data/TB_Burden_Country_clean.csv ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"\\nüéì Report Generation:\")\n",
    "export_to_pdf()\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis Complete!\")\n",
    "print(f\"   ‚Ä¢ Author: Daniel Wanjal Machimbo\")\n",
    "print(f\"   ‚Ä¢ Course: MCSC 2108 Data Visualization\") \n",
    "print(f\"   ‚Ä¢ Institution: The Cooperative University of Kenya\")\n",
    "print(f\"   ‚Ä¢ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
