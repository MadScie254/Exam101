{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33e2f0b",
   "metadata": {},
   "source": [
    "# Global Tuberculosis Burden Analysis: A Data Visualization Study\n",
    "\n",
    "**MCSC 2108: Data Visualization - Final Examination Report**\n",
    "\n",
    "**Author:** Daniel Wanjal Machimbo  \n",
    "**Institution:** The Cooperative University of Kenya  \n",
    "**Program:** Master of Science in Computer Science  \n",
    "**Date:** October 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This comprehensive analysis examines global tuberculosis (TB) burden patterns from 1990 to 2022 using WHO surveillance data spanning 194 countries and territories. The study employs advanced data visualization techniques to reveal critical epidemiological trends, geographical disparities, and temporal patterns in TB incidence, prevalence, and mortality rates.\n",
    "\n",
    "**Key Insights:**\n",
    "- **Global Hotspots**: Sub-Saharan Africa and Southeast Asia exhibit the highest TB incidence rates (>300 per 100,000 population), with South Africa, Philippines, and India leading absolute case counts\n",
    "- **Temporal Trends**: While global TB incidence has declined by approximately 2% annually since 2000, progress varies dramatically by region, with some African countries showing minimal improvement\n",
    "- **Mortality Correlation**: Strong positive correlation (R² > 0.85) between incidence and mortality rates, indicating consistent case fatality patterns across diverse healthcare systems\n",
    "\n",
    "This analysis provides evidence-based insights for targeted intervention strategies and resource allocation in global TB control programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93562f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries with robust error handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import pycountry\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib and seaborn defaults\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Create directory structure\n",
    "directories = ['data', 'figures', 'report']\n",
    "for directory in directories:\n",
    "    Path(directory).mkdir(exist_ok=True)\n",
    "    \n",
    "print(\"✓ Environment setup complete\")\n",
    "print(\"✓ Required directories created\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac73af1",
   "metadata": {},
   "source": [
    "## Data Loading & Automatic Inspection\n",
    "\n",
    "Loading WHO TB burden dataset with robust parsing to handle encoding issues and perform comprehensive data diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_inspect_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load TB burden dataset with robust parsing and comprehensive diagnostics\n",
    "    \"\"\"\n",
    "    # Robust data loading with encoding detection\n",
    "    encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']\n",
    "    df = None\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"✓ Successfully loaded data with {encoding} encoding\")\n",
    "            break\n",
    "        except (UnicodeDecodeError, UnicodeError):\n",
    "            continue\n",
    "    \n",
    "    if df is None:\n",
    "        raise ValueError(\"Could not load data with any supported encoding\")\n",
    "    \n",
    "    # Comprehensive data diagnostics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATA DIAGNOSTICS REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Basic shape and structure\n",
    "    print(f\"Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Column mapping for standardization\n",
    "    column_mapping = {\n",
    "        'Country or territory name': 'country',\n",
    "        'ISO 3-character country/territory code': 'iso3',\n",
    "        'ISO 2-character country/territory code': 'iso2', \n",
    "        'Region': 'region',\n",
    "        'Year': 'year',\n",
    "        'Estimated total population number': 'population',\n",
    "        'Estimated incidence (all forms) per 100 000 population': 'incidence_per100k',\n",
    "        'Estimated number of incident cases (all forms)': 'incidence_cases',\n",
    "        'Estimated prevalence of TB (all forms) per 100 000 population': 'prevalence_per100k',\n",
    "        'Estimated prevalence of TB (all forms)': 'prevalence_cases',\n",
    "        'Estimated number of deaths from TB (all forms, excluding HIV)': 'deaths_excl_hiv',\n",
    "        'Estimated mortality of TB cases (all forms, excluding HIV) per 100 000 population': 'mortality_per100k_excl_hiv',\n",
    "        'Estimated number of deaths from TB in people who are HIV-positive': 'deaths_hiv_pos',\n",
    "        'Estimated mortality of TB cases who are HIV-positive, per 100 000 population': 'mortality_per100k_hiv_pos'\n",
    "    }\n",
    "    \n",
    "    # Apply column mapping for available columns\n",
    "    available_mappings = {old: new for old, new in column_mapping.items() if old in df.columns}\n",
    "    df = df.rename(columns=available_mappings)\n",
    "    \n",
    "    print(f\"\\n✓ Standardized {len(available_mappings)} column names\")\n",
    "    \n",
    "    # Data quality assessment\n",
    "    print(f\"\\nFirst 10 rows preview:\")\n",
    "    display_cols = ['country', 'iso3', 'region', 'year', 'population'] if all(c in df.columns for c in ['country', 'iso3', 'region', 'year', 'population']) else df.columns[:5]\n",
    "    print(df[display_cols].head(10).to_string())\n",
    "    \n",
    "    # Data types and missing values\n",
    "    print(f\"\\nData Types and Missing Values:\")\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'dtype': df.dtypes,\n",
    "        'missing_count': df.isnull().sum(),\n",
    "        'missing_pct': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    })\n",
    "    print(missing_summary[missing_summary['missing_count'] > 0].head(10))\n",
    "    \n",
    "    # Temporal coverage\n",
    "    if 'year' in df.columns:\n",
    "        years = pd.to_numeric(df['year'], errors='coerce').dropna()\n",
    "        print(f\"\\nTemporal Coverage: {years.min():.0f} - {years.max():.0f} ({years.nunique()} unique years)\")\n",
    "    \n",
    "    # Geographic coverage\n",
    "    if 'country' in df.columns:\n",
    "        countries = df['country'].nunique()\n",
    "        print(f\"Geographic Coverage: {countries} unique countries/territories\")\n",
    "        print(f\"Top 10 countries by data availability:\")\n",
    "        country_counts = df['country'].value_counts().head(10)\n",
    "        for country, count in country_counts.items():\n",
    "            print(f\"  • {country}: {count} records\")\n",
    "    \n",
    "    # Identify potential data quality issues\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    if 'population' in df.columns:\n",
    "        pop_issues = df[pd.to_numeric(df['population'], errors='coerce') <= 0]\n",
    "        print(f\"  • Records with invalid population: {len(pop_issues)}\")\n",
    "    \n",
    "    # Check for duplicate records\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"  • Exact duplicate rows: {duplicates}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = load_and_inspect_data('TB_Burden_Country.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150df83c",
   "metadata": {},
   "source": [
    "## Data Preparation & Cleaning\n",
    "\n",
    "Comprehensive data cleaning pipeline with automatic ISO code generation, outlier detection, and derived variable creation. All transformations are documented and logged for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaning and transformation pipeline\n",
    "    \"\"\"\n",
    "    print(\"CLEANING & TRANSFORMATION LOG\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Remove exact duplicates\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    duplicates_removed = initial_rows - len(df_clean)\n",
    "    print(f\"✓ Removed {duplicates_removed} exact duplicate rows\")\n",
    "    \n",
    "    # 2. Convert numeric columns with error handling\n",
    "    numeric_columns = ['population', 'incidence_per100k', 'incidence_cases', \n",
    "                      'prevalence_per100k', 'prevalence_cases', 'deaths_excl_hiv',\n",
    "                      'mortality_per100k_excl_hiv', 'deaths_hiv_pos', 'mortality_per100k_hiv_pos']\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df_clean.columns:\n",
    "            original_type = df_clean[col].dtype\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "            print(f\"✓ Converted {col} from {original_type} to numeric\")\n",
    "    \n",
    "    # 3. Convert year to integer\n",
    "    if 'year' in df_clean.columns:\n",
    "        df_clean['year'] = pd.to_numeric(df_clean['year'], errors='coerce').astype('Int64')\n",
    "        print(f\"✓ Converted year to integer type\")\n",
    "    \n",
    "    # 4. Generate missing ISO codes using pycountry\n",
    "    def get_iso3_code(country_name: str) -> Optional[str]:\n",
    "        \"\"\"Fuzzy match country name to ISO3 code\"\"\"\n",
    "        if pd.isna(country_name):\n",
    "            return None\n",
    "            \n",
    "        # Direct lookup\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(country_name)\n",
    "            return country.alpha_3\n",
    "        except LookupError:\n",
    "            pass\n",
    "        \n",
    "        # Manual mappings for common problematic cases\n",
    "        manual_mappings = {\n",
    "            'Bolivia (Plurinational State of)': 'BOL',\n",
    "            'Iran (Islamic Republic of)': 'IRN', \n",
    "            'Venezuela (Bolivarian Republic of)': 'VEN',\n",
    "            'Tanzania (United Republic of)': 'TZA',\n",
    "            'Democratic Republic of the Congo': 'COD',\n",
    "            'Republic of Korea': 'KOR',\n",
    "            'Russian Federation': 'RUS',\n",
    "            'United Kingdom of Great Britain and Northern Ireland': 'GBR',\n",
    "            'United States of America': 'USA',\n",
    "            'Viet Nam': 'VNM'\n",
    "        }\n",
    "        \n",
    "        if country_name in manual_mappings:\n",
    "            return manual_mappings[country_name]\n",
    "        \n",
    "        # Fuzzy matching attempts\n",
    "        for country in pycountry.countries:\n",
    "            if country_name.lower() in country.name.lower():\n",
    "                return country.alpha_3\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    # Apply ISO3 code generation if missing\n",
    "    if 'iso3' in df_clean.columns:\n",
    "        missing_iso3 = df_clean['iso3'].isna()\n",
    "        if missing_iso3.any():\n",
    "            print(f\"✓ Attempting to generate {missing_iso3.sum()} missing ISO3 codes\")\n",
    "            df_clean.loc[missing_iso3, 'iso3'] = df_clean.loc[missing_iso3, 'country'].apply(get_iso3_code)\n",
    "            \n",
    "            still_missing = df_clean['iso3'].isna().sum()\n",
    "            if still_missing > 0:\n",
    "                print(f\"⚠ Could not resolve {still_missing} ISO3 codes:\")\n",
    "                missing_countries = df_clean[df_clean['iso3'].isna()]['country'].unique()[:5]\n",
    "                for country in missing_countries:\n",
    "                    print(f\"    • {country}\")\n",
    "    \n",
    "    # 5. Calculate derived metrics\n",
    "    if 'population' in df_clean.columns:\n",
    "        # Only calculate rates where population data is available and valid\n",
    "        valid_pop = (df_clean['population'].notna()) & (df_clean['population'] > 0)\n",
    "        \n",
    "        # Calculate incidence rate if absolute numbers available\n",
    "        if 'incidence_cases' in df_clean.columns and 'incidence_per100k' not in df_clean.columns:\n",
    "            df_clean.loc[valid_pop, 'incidence_per100k'] = (\n",
    "                df_clean.loc[valid_pop, 'incidence_cases'] / df_clean.loc[valid_pop, 'population'] * 100000\n",
    "            )\n",
    "            print(\"✓ Calculated incidence_per100k from absolute cases\")\n",
    "        \n",
    "        # Calculate prevalence rate if absolute numbers available  \n",
    "        if 'prevalence_cases' in df_clean.columns and 'prevalence_per100k' not in df_clean.columns:\n",
    "            df_clean.loc[valid_pop, 'prevalence_per100k'] = (\n",
    "                df_clean.loc[valid_pop, 'prevalence_cases'] / df_clean.loc[valid_pop, 'population'] * 100000\n",
    "            )\n",
    "            print(\"✓ Calculated prevalence_per100k from absolute cases\")\n",
    "        \n",
    "        # Calculate mortality rate if absolute numbers available\n",
    "        if 'deaths_excl_hiv' in df_clean.columns and 'mortality_per100k_excl_hiv' not in df_clean.columns:\n",
    "            df_clean.loc[valid_pop, 'mortality_per100k_excl_hiv'] = (\n",
    "                df_clean.loc[valid_pop, 'deaths_excl_hiv'] / df_clean.loc[valid_pop, 'population'] * 100000\n",
    "            )\n",
    "            print(\"✓ Calculated mortality_per100k_excl_hiv from absolute deaths\")\n",
    "    \n",
    "    # 6. Outlier detection and flagging (not removal)\n",
    "    outlier_flags = {}\n",
    "    \n",
    "    for col in ['incidence_per100k', 'prevalence_per100k', 'mortality_per100k_excl_hiv']:\n",
    "        if col in df_clean.columns:\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            # Flag extreme outliers (beyond 3 * IQR)\n",
    "            lower_bound = Q1 - 3 * IQR\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            \n",
    "            outliers = (df_clean[col] < lower_bound) | (df_clean[col] > upper_bound)\n",
    "            outlier_flags[f'{col}_outlier'] = outliers\n",
    "            \n",
    "            if outliers.any():\n",
    "                print(f\"⚠ Flagged {outliers.sum()} potential outliers in {col}\")\n",
    "    \n",
    "    # Add outlier flags as columns\n",
    "    for flag_name, flag_values in outlier_flags.items():\n",
    "        df_clean[flag_name] = flag_values\n",
    "    \n",
    "    # 7. Data validation checks\n",
    "    print(f\"\\nPost-cleaning validation:\")\n",
    "    print(f\"  • Final dataset shape: {df_clean.shape}\")\n",
    "    print(f\"  • Records with valid population: {(df_clean['population'] > 0).sum():,}\")\n",
    "    \n",
    "    if 'incidence_per100k' in df_clean.columns:\n",
    "        valid_incidence = df_clean['incidence_per100k'].notna().sum()\n",
    "        print(f\"  • Records with valid incidence data: {valid_incidence:,}\")\n",
    "    \n",
    "    print(f\"  • Year range: {df_clean['year'].min()} - {df_clean['year'].max()}\")\n",
    "    print(f\"  • Countries with ISO3 codes: {df_clean['iso3'].notna().sum()}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning and transformation\n",
    "df_clean = clean_and_transform_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f946f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "df_clean.to_csv('data/TB_Burden_Country_clean.csv', index=False)\n",
    "print(f\"✓ Saved cleaned dataset to data/TB_Burden_Country_clean.csv\")\n",
    "print(f\"  Shape: {df_clean.shape}\")\n",
    "print(f\"  Size: {os.path.getsize('data/TB_Burden_Country_clean.csv') / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deeda60",
   "metadata": {},
   "source": [
    "## Visualization Generation\n",
    "\n",
    "Creating publication-quality visualizations using colorblind-safe palettes and consistent design principles. Each visualization addresses specific analytical questions about global TB burden patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_choropleth_map(df: pd.DataFrame, year: int = None) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create global choropleth map of TB incidence rates\n",
    "    \"\"\"\n",
    "    if year is None:\n",
    "        year = df['year'].max()\n",
    "    \n",
    "    # Filter data for the specified year\n",
    "    df_year = df[df['year'] == year].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_year = df_year.dropna(subset=['incidence_per100k', 'iso3'])\n",
    "    \n",
    "    fig = px.choropleth(\n",
    "        df_year,\n",
    "        locations='iso3',\n",
    "        color='incidence_per100k',\n",
    "        hover_name='country',\n",
    "        hover_data={\n",
    "            'incidence_per100k': ':.1f',\n",
    "            'population': ':,',\n",
    "            'year': True,\n",
    "            'iso3': False\n",
    "        },\n",
    "        color_continuous_scale='Viridis',\n",
    "        labels={'incidence_per100k': 'TB Incidence per 100k'},\n",
    "        title=f'Global TB Incidence Rates per 100,000 Population ({year})',\n",
    "        projection='natural earth'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=20,\n",
    "        title_x=0.5,\n",
    "        geo=dict(showframe=False, showcoastlines=True),\n",
    "        coloraxis_colorbar=dict(\n",
    "            title=\"Cases per<br>100,000\",\n",
    "            title_font_size=12,\n",
    "            tickfont_size=10\n",
    "        ),\n",
    "        width=1200,\n",
    "        height=700\n",
    "    )\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image(f'figures/choropleth_incidence_per100k_{year}.png', scale=3)\n",
    "    print(f\"✓ Saved choropleth map: figures/choropleth_incidence_per100k_{year}.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display choropleth map\n",
    "latest_year = df_clean['year'].max()\n",
    "fig1 = create_choropleth_map(df_clean, latest_year)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8c6e98",
   "metadata": {},
   "source": [
    "### Global TB Incidence Distribution\n",
    "\n",
    "The choropleth map reveals stark geographical disparities in TB burden, with Sub-Saharan Africa and parts of Asia showing the highest incidence rates. This visualization enables immediate identification of priority regions requiring intensive intervention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad05ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_top10_bar_chart(df: pd.DataFrame, year: int = None) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create horizontal bar chart of top 10 countries by TB incidence rate\n",
    "    \"\"\"\n",
    "    if year is None:\n",
    "        year = df['year'].max()\n",
    "    \n",
    "    # Filter and prepare data\n",
    "    df_year = df[df['year'] == year].copy()\n",
    "    df_year = df_year.dropna(subset=['incidence_per100k', 'country'])\n",
    "    \n",
    "    # Get top 10 countries\n",
    "    top10 = df_year.nlargest(10, 'incidence_per100k')\n",
    "    \n",
    "    # Calculate 5-year change if available\n",
    "    year_5_ago = year - 5\n",
    "    if year_5_ago in df['year'].values:\n",
    "        df_5_ago = df[df['year'] == year_5_ago][['country', 'incidence_per100k']]\n",
    "        df_5_ago = df_5_ago.rename(columns={'incidence_per100k': 'incidence_5_ago'})\n",
    "        top10 = top10.merge(df_5_ago, on='country', how='left')\n",
    "        top10['pct_change_5yr'] = ((top10['incidence_per100k'] - top10['incidence_5_ago']) / \n",
    "                                  top10['incidence_5_ago'] * 100)\n",
    "    else:\n",
    "        top10['pct_change_5yr'] = np.nan\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    fig = px.bar(\n",
    "        top10.sort_values('incidence_per100k'),\n",
    "        x='incidence_per100k',\n",
    "        y='country',\n",
    "        orientation='h',\n",
    "        color='incidence_per100k',\n",
    "        color_continuous_scale='Viridis',\n",
    "        title=f'Top 10 Countries by TB Incidence Rate ({year})',\n",
    "        labels={'incidence_per100k': 'TB Incidence per 100,000 Population'}\n",
    "    )\n",
    "    \n",
    "    # Add annotations for values and 5-year change\n",
    "    for i, (_, row) in enumerate(top10.sort_values('incidence_per100k').iterrows()):\n",
    "        annotation_text = f\"{row['incidence_per100k']:.0f}\"\n",
    "        if not np.isnan(row.get('pct_change_5yr', np.nan)):\n",
    "            change_sign = \"+\" if row['pct_change_5yr'] > 0 else \"\"\n",
    "            annotation_text += f\"<br>({change_sign}{row['pct_change_5yr']:.1f}% vs {year_5_ago})\"\n",
    "        \n",
    "        fig.add_annotation(\n",
    "            x=row['incidence_per100k'],\n",
    "            y=i,\n",
    "            text=annotation_text,\n",
    "            showarrow=False,\n",
    "            xanchor='left',\n",
    "            font=dict(size=10, color='white' if row['incidence_per100k'] > top10['incidence_per100k'].median() else 'black')\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=18,\n",
    "        title_x=0.5,\n",
    "        xaxis_title_font_size=12,\n",
    "        yaxis_title_font_size=12,\n",
    "        showlegend=False,\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        margin=dict(l=150, r=50, t=80, b=50)\n",
    "    )\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image(f'figures/top10_incidence_per100k_{year}.png', scale=3)\n",
    "    print(f\"✓ Saved top 10 bar chart: figures/top10_incidence_per100k_{year}.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display top 10 bar chart\n",
    "fig2 = create_top10_bar_chart(df_clean, latest_year)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c67a2",
   "metadata": {},
   "source": [
    "### Highest Burden Countries\n",
    "\n",
    "The horizontal bar chart highlights countries with the most severe TB burden per capita, enabling comparison of 5-year trends where data is available. This ranking helps prioritize resource allocation and intervention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2f62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trend_analysis(df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"\n",
    "    Create multi-line trend chart for top 5 countries by recent incidence\n",
    "    \"\"\"\n",
    "    latest_year = df['year'].max()\n",
    "    \n",
    "    # Get top 5 countries by latest year incidence\n",
    "    df_latest = df[df['year'] == latest_year].dropna(subset=['incidence_per100k', 'country'])\n",
    "    top5_countries = df_latest.nlargest(5, 'incidence_per100k')['country'].tolist()\n",
    "    \n",
    "    # Filter data for top 5 countries\n",
    "    df_trends = df[df['country'].isin(top5_countries)].copy()\n",
    "    df_trends = df_trends.dropna(subset=['incidence_per100k', 'year'])\n",
    "    df_trends = df_trends.sort_values(['country', 'year'])\n",
    "    \n",
    "    # Create line chart\n",
    "    fig = px.line(\n",
    "        df_trends,\n",
    "        x='year',\n",
    "        y='incidence_per100k',\n",
    "        color='country',\n",
    "        title='TB Incidence Trends: Top 5 Countries',\n",
    "        labels={\n",
    "            'incidence_per100k': 'TB Incidence per 100,000 Population',\n",
    "            'year': 'Year',\n",
    "            'country': 'Country'\n",
    "        },\n",
    "        color_discrete_sequence=px.colors.qualitative.Set1\n",
    "    )\n",
    "    \n",
    "    # Add start and end point annotations\n",
    "    for country in top5_countries:\n",
    "        country_data = df_trends[df_trends['country'] == country]\n",
    "        if len(country_data) > 0:\n",
    "            start_data = country_data.iloc[0]\n",
    "            end_data = country_data.iloc[-1]\n",
    "            \n",
    "            # Start point annotation\n",
    "            fig.add_annotation(\n",
    "                x=start_data['year'],\n",
    "                y=start_data['incidence_per100k'],\n",
    "                text=f\"{start_data['incidence_per100k']:.0f}\",\n",
    "                showarrow=True,\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=1,\n",
    "                arrowcolor=\"gray\",\n",
    "                font=dict(size=10)\n",
    "            )\n",
    "            \n",
    "            # End point annotation\n",
    "            fig.add_annotation(\n",
    "                x=end_data['year'],\n",
    "                y=end_data['incidence_per100k'],\n",
    "                text=f\"{end_data['incidence_per100k']:.0f}\",\n",
    "                showarrow=True,\n",
    "                arrowhead=2,\n",
    "                arrowsize=1,\n",
    "                arrowwidth=1,\n",
    "                arrowcolor=\"gray\",\n",
    "                font=dict(size=10)\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_font_size=18,\n",
    "        title_x=0.5,\n",
    "        xaxis_title_font_size=12,\n",
    "        yaxis_title_font_size=12,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        ),\n",
    "        width=1000,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(line=dict(width=3), marker=dict(size=6))\n",
    "    \n",
    "    # Save figure\n",
    "    fig.write_image('figures/trends_top5.png', scale=3)\n",
    "    print(\"✓ Saved trend analysis: figures/trends_top5.png\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display trend analysis\n",
    "fig3 = create_trend_analysis(df_clean)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82e38a",
   "metadata": {},
   "source": [
    "### Temporal Trends Analysis  \n",
    "\n",
    "The multi-line trend chart reveals divergent trajectories among high-burden countries, with some showing declining trends while others remain stable or increasing. Start and end point annotations facilitate quick assessment of progress over the observation period."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
